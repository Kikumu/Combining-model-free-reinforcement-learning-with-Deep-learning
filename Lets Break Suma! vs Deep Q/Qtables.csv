Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4353, 0.4392, 0.4353],
         [0.5176, 0.5176, 0.5176,  ..., 0.4353, 0.4392, 0.4353],
         [0.5176, 0.5176, 0.5176,  ..., 0.4431, 0.4431, 0.4392],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6353, 0.6353, 0.6431],
         [0.1647, 0.1647, 0.1647,  ..., 0.6275, 0.6353, 0.6431],
         [0.1647, 0.1647, 0.1647,  ..., 0.6196, 0.6275, 0.6431]]]), action=tensor([5]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([1]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([4]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([8]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([2]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([1]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([7]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([6]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([4]), next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), reward=tensor([-4.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([1]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([0]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([7]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([8]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([8]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([4]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([1]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([1]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([5]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([3]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([4]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([8]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([6]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([7]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([1]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([7]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([7]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([3]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([5]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([2]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([6]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([4]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([6]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([2]), next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), reward=tensor([-4.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([8]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([3]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([7]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([1]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([7]), next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), reward=tensor([-4.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([3]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([7]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([2]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([2]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([1]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([2]), next_state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), reward=tensor([-1.]))
Experience(state=tensor([[[0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4118, 0.4157, 0.4157],
         [0.5176, 0.5176, 0.5176,  ..., 0.4157, 0.4196, 0.4196],
         ...,
         [0.1647, 0.1647, 0.1647,  ..., 0.6039, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5961, 0.6078, 0.6157],
         [0.1647, 0.1647, 0.1647,  ..., 0.5922, 0.6000, 0.6157]]]), action=tensor([8]), next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), reward=tensor([-4.]))
