{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from mss import mss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pyautogui\n",
    "import time\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Extraction(Screen Extraction)\n",
    "from PIL import ImageGrab\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "white_template = cv2.imread('templates/white_temp.png',0) \n",
    "blue_template = cv2.imread('templates/blue_temp.png',0)\n",
    "orange_template = cv2.imread('templates/orange_temp.png',0)\n",
    "green_template = cv2.imread('templates/green_temp.png',0)\n",
    "#cv2.imshow('image',green_template)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial block count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of blocks 27\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "#one scrren\n",
    "bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} #top left corner of screen\n",
    "sct = mss()\n",
    "sct_img = np.array(sct.grab(bounding_box))\n",
    "gray = cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#gray_template = cv2.cvtColor(green_template, cv2.COLOR_BGR2GRAY)\n",
    "#np_scrn = np.array(sct_img)\n",
    "\n",
    "#print( gray_template.shape[::-1])\n",
    "w,h = blue_template.shape[::-1]  #here\n",
    "result = cv2.matchTemplate(gray,blue_template, cv2.TM_CCOEFF_NORMED) #here\n",
    "\n",
    "f = set()\n",
    "loc = np.where(result >=0.9)\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv2.rectangle(sct_img,pt,(pt[0] + w, pt[1] + h),(0,255,0),3)\n",
    "    sensitivity = 100\n",
    "    f.add((round(pt[0]/sensitivity), round(pt[1]/sensitivity)))\n",
    "    \n",
    "found_count = len(f)    \n",
    "print(\"amount of blocks\", found_count)\n",
    "cv2.imshow(\"result\", sct_img)\n",
    "#cv2.imshow('screen', np.array(sct_img))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "initial_block_value = found_count\n",
    "print(initial_block_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#screen method 1\n",
    "#img = ImageGrab.grab(bbox=(200,630,850,600)) #bbox specifies specific region (bbox= x,y,width,height *starts top-left)\n",
    "#img_np = np.array(img) #this is the array obtained from conversion\n",
    "#frame = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
    "#cv2.imshow(\"test\", frame)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#one scrren\n",
    "bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} #top left corner of screen\n",
    "sct = mss()\n",
    "#while True:\n",
    "#7 outputs(mouse positions)\n",
    "sct_img = sct.grab(bounding_box)\n",
    "cv2.imshow('screen', np.array(sct_img))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#screen method 2(Current)\n",
    "bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} #top left corner of screen\n",
    "sct = mss()\n",
    "while True:\n",
    "    #7 outputs(mouse positions)\n",
    "    sct_img = sct.grab(bounding_box)\n",
    "    cv2.imshow('screen', np.array(sct_img))\n",
    "    if (cv2.waitKey(1) & 0xFF) == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 outputs\n",
    "def take_actions(action):\n",
    "    if action == 0:\n",
    "        pyautogui.moveTo(640, 200, duration = 0.03) \n",
    "    elif action == 1:\n",
    "        pyautogui.moveTo(740, 200, duration = 0.03) \n",
    "    elif action == 2:\n",
    "        pyautogui.moveTo(840, 200, duration = 0.03)\n",
    "    elif action ==3:\n",
    "        pyautogui.moveTo(940, 200, duration = 0.03) \n",
    "    elif action ==4:\n",
    "        pyautogui.moveTo(1040, 200, duration = 0.03)\n",
    "    elif action ==5:\n",
    "        pyautogui.moveTo(1140, 200, duration = 0.03)\n",
    "    elif action ==6:\n",
    "        pyautogui.moveTo(1240, 200, duration = 0.03)\n",
    "    elif action == 7:\n",
    "        pyautogui.moveTo(1340, 200, duration = 0.03) \n",
    "    elif action == 8:\n",
    "        pyautogui.moveTo(1440, 200, duration = 0.03)\n",
    "    elif action == 9:\n",
    "        pyautogui.moveTo(1500, 200, duration = 0.03)\n",
    "    else:\n",
    "        print(\"Not allowed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=5, action=6, next_state=7, reward=8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Xp = namedtuple('Experience',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "Xp_points = Xp(5,6,7,8)\n",
    "Xp_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------DQN(Fully connected)-----------------------------------------#\n",
    "class DQN (nn.Module):\n",
    "    def __init__(self,img_height, img_width):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(in_features = img_height*img_width*3,out_features=124)\n",
    "        self.layer2      = nn.Linear(in_features = 124, out_features = 132)\n",
    "        self.layer3      = nn.Linear(in_features = 132, out_features = 132)\n",
    "        self.layer4      = nn.Linear(in_features = 132, out_features = 100)\n",
    "        self.output      = nn.Linear(in_features = 100, out_features = 10)\n",
    "#--------------------forward pass---------------------------------------------------------------------#\n",
    "    def forward(self,t):\n",
    "        t = t.flatten(start_dim = 1) #select first layer and flatten\n",
    "        t = F.relu(self.input_layer(t))\n",
    "        t = F.relu(self.layer2(t))\n",
    "        t = F.relu(self.layer3(t))\n",
    "        t = F.relu(self.layer4(t))\n",
    "        t = self.output(t)\n",
    "        #print(t.item())\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate and network setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = 0.01\n",
    "#policy_net = DQN(600,850).to(device = \"cpu\")\n",
    "#target_net = DQN(600,850).to(device = \"cpu\")\n",
    "#target_net.load_state_dict(policy_net.state_dict()) #set weights to be the same\n",
    "#target_net.eval() #not in training mode\n",
    "#optimizer = optim.Adam(params = policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q values setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Values():\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):\n",
    "        #print(\"nx\",next_states)\n",
    "        #print(\"shape\", next_states.shape[0])\n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        #print(\"ft\",final_state_locations)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        #print(\"nf\",non_final_state_locations)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(Q_Values.device)\n",
    "        #values = torch.zeros(2).to(Q_Values.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    batch = Xp(*zip(*experiences))\n",
    "    t1 = torch.cat(batch.state)\n",
    "    #print(\"t1\",t1)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    #print(\"t2\",t2)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    #print(\"t3\",t3)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return t1,t2,t3,t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------DEJAVU----------------------------------------------------------#\n",
    "class ReplayMemory():\n",
    "    def __init__(self,capacity):   \n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            #overwrite old memory\n",
    "            self.memory[self.push_count%self.capacity] = experience\n",
    "        self.push_count+=1\n",
    "    #number of returned memories will be equal to batch size\n",
    "    def sample(self, batch_size):\n",
    "        return rand.sample(self.memory,batch_size)\n",
    "    #sample must be equal to batch size\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory)>=batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon Decay Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Greed():\n",
    "    def __init__(self,start,end,decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    def get_exploration_rate(self,current_step):\n",
    "        return self.end + (self.start - self.end)*math.exp(-1. * current_step*self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_screen_data(screen):\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32)/255\n",
    "        screen = torch.from_numpy(screen).float()\n",
    "        \n",
    "        resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize((600, 850)),\n",
    "                    T.ToTensor()])\n",
    "        return resize(screen).unsqueeze(0).to(device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(100000)\n",
    "gamma = 0.7 #affects network\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,h = blue_template.shape[::-1]  #set template shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "policy_net = DQN(600,850).to(device = \"cpu\")\n",
    "target_net = DQN(600,850).to(device = \"cpu\")\n",
    "target_net.load_state_dict(policy_net.state_dict()) #set weights to be the same\n",
    "target_net.eval() #not in training mode\n",
    "optimizer = optim.Adam(params = policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_value tensor([5])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([1])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([7])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([9])\n",
      "updated policy\n",
      "rand_value tensor([2])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([3])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([9])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([2])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([2])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([6])\n",
      "rand_value tensor([6])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([9])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([0.])\n",
      "net_value tensor([8])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([1])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([2])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([2])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([8])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([2])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n",
      "current_blocks 25 past_blocks 26\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([1])\n",
      "rand_value tensor([3])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([2])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([6])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([8])\n",
      "net_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([7])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n",
      "rand_value tensor([7])\n",
      "reward_tensor tensor([0.])\n",
      "net_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "net_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([2])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([7])\n",
      "reward_tensor tensor([0.])\n",
      "net_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([7])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([7])\n",
      "current_blocks 25 past_blocks 26\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([4])\n",
      "current_blocks 23 past_blocks 25\n",
      "sub 2\n",
      "reward_tensor tensor([2.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([6])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([1])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([6])\n",
      "current_blocks 25 past_blocks 26\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([0.])\n",
      "net_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "net_value tensor([8])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([9])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "net_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([9])\n",
      "rand_value tensor([6])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([6])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "net_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([3])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([3])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([0.])\n",
      "net_value tensor([8])\n",
      "rand_value tensor([9])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([0.])\n",
      "net_value tensor([8])\n",
      "current_blocks 26 past_blocks 27\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([6])\n",
      "reward_tensor tensor([0.])\n",
      "net_value tensor([8])\n",
      "current_blocks 25 past_blocks 26\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([1])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([1])\n",
      "reward_tensor tensor([0.])\n",
      "net_value tensor([8])\n",
      "current_blocks 24 past_blocks 25\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([8])\n",
      "current_blocks 23 past_blocks 24\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([2])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([2])\n",
      "current_blocks 22 past_blocks 23\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([7])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n",
      "current_blocks 21 past_blocks 22\n",
      "sub 1\n",
      "reward_tensor tensor([1.])\n",
      "rand_value tensor([6])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([7])\n",
      "reward_tensor tensor([0.])\n",
      "rand_value tensor([4])\n"
     ]
    },
    {
     "ename": "FailSafeException",
     "evalue": "PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailSafeException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-c7d11e8a3775>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyautogui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mpyautogui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m###############################################STATE########################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyautogui\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[0mfuncArgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcallargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrappedFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mfailSafeCheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m         \u001b[0mreturnVal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrappedFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0m_handlePause\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuncArgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_pause\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyautogui\\__init__.py\u001b[0m in \u001b[0;36mfailSafeCheck\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1670\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mFAILSAFE\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFAILSAFE_POINTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m         raise FailSafeException(\n\u001b[1;32m-> 1672\u001b[1;33m             \u001b[1;34m\"PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1673\u001b[0m         )\n\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailSafeException\u001b[0m: PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED."
     ]
    }
   ],
   "source": [
    "#def launch():\n",
    "exp_rate = 1\n",
    "episode = 0\n",
    "max_rate = 1\n",
    "min_rate = 0.01\n",
    "exp_decay = 0.01\n",
    "for n in range (200):\n",
    "    w,h = blue_template.shape[::-1]  #set template shape\n",
    "    count = 0\n",
    "    current_blocks = initial_block_value\n",
    "    past_blocks = initial_block_value\n",
    "    reward = 0\n",
    "    bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} \n",
    "    sct = mss()\n",
    "    sct_img =(sct.grab(bounding_box))\n",
    "    first_example = transform_screen_data(sct_img) #Transform screen(current screen)/previous\n",
    "    sct_img =(sct.grab(bounding_box))\n",
    "    second_example = transform_screen_data(sct_img) #Transform screen(current screen)/previous\n",
    "    sct_img =(sct.grab(bounding_box))\n",
    "    third_example = transform_screen_data(sct_img) #Transform screen(current screen)/previous\n",
    "    example = first_example - second_example - third_example\n",
    "    while True:\n",
    "        #10 outputs(mouse positions)\n",
    "        init_random = np.random.uniform(0,1)\n",
    "        past_blocks = current_blocks\n",
    "        count+=1\n",
    "        x,y = pyautogui.position()\n",
    "        pyautogui.click(x, y)\n",
    "###############################################STATE########################################################        \n",
    "        state = example\n",
    "###############################################ACTION########################################################\n",
    "        #epsilon greed\n",
    "        exp_thresh = rand.uniform(0,1)\n",
    "        if exp_thresh > exp_rate:\n",
    "             with torch.no_grad():#need to review this\n",
    "                action = policy_net(example).argmax(dim=1).to(device =\"cpu\")\n",
    "                print(\"net_value\", action)\n",
    "        else:\n",
    "            rndm = rand.randrange(1,10)\n",
    "            action = torch.tensor([rndm]).to(device = \"cpu\")\n",
    "            print(\"rand_value\", action)\n",
    "        take_actions(action)\n",
    "        \n",
    "#############################################REWARD###########################################################        \n",
    "        sct_img =(sct.grab(bounding_box))\n",
    "        gray = cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGR2GRAY) #block detection\n",
    "        f = set()\n",
    "        result = cv2.matchTemplate(gray,blue_template, cv2.TM_CCOEFF_NORMED) #match template per frame\n",
    "        loc = np.where(result >=0.9)\n",
    "        for pt in zip(*loc[::-1]):\n",
    "            sensitivity = 100\n",
    "            f.add((round(pt[0]/sensitivity), round(pt[1]/sensitivity)))\n",
    "        found_count = len(f) # reward factor\n",
    "        current_blocks = found_count\n",
    "        #reward feed to network(onl give reard if resulting action is not a black screen)\n",
    "        if np.average(gray) > 20:\n",
    "            if(current_blocks < past_blocks):\n",
    "                print(\"current_blocks\", current_blocks,\"past_blocks\",past_blocks )\n",
    "                reward =(past_blocks - current_blocks)\n",
    "                print(\"sub\",reward)\n",
    "                reward_tensor = torch.FloatTensor([reward], device = \"cpu\")\n",
    "                print(\"reward_tensor\",reward_tensor)\n",
    "            else:\n",
    "                reward = 0\n",
    "                reward_tensor = torch.FloatTensor([reward], device = \"cpu\")\n",
    "                print(\"reward_tensor\",reward_tensor)\n",
    "            \n",
    "        #from that action i got this state\n",
    "        sct_img =(sct.grab(bounding_box))\n",
    "        gray1 = cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGR2GRAY) #block detection\n",
    "#########################################NEXT_STATE############################################################        \n",
    "        # if resulting action is a black screen\n",
    "        if(np.average(gray) < 20):\n",
    "            #reward = 0\n",
    "            #current_blocks = 0\n",
    "            #past_blocks = 0\n",
    "            #reward_tensor = torch.FloatTensor([reward], device = \"cpu\")\n",
    "            screen_return = torch.zeros_like(state)\n",
    "        else:\n",
    "            second_example1 = transform_screen_data(sct_img)\n",
    "            sct_img =(sct.grab(bounding_box))\n",
    "            second_example2 = transform_screen_data(sct_img)\n",
    "            sct_img =(sct.grab(bounding_box))\n",
    "            second_example3 = transform_screen_data(sct_img)\n",
    "            screen_return = second_example1 - second_example2 - second_example3\n",
    "        next_state = screen_return\n",
    "        memory.push(Xp(state,action,next_state,reward_tensor))\n",
    "        state = next_state\n",
    "######################################EXPERIENCE AND Q VALUES##################################################\n",
    "        if memory.can_provide_sample(200):\n",
    "            Xps = memory.sample(200)\n",
    "            states, actions,rewards,next_states = extract_tensors(Xps)\n",
    "            current_q_values = Q_Values.get_current(policy_net,states,actions)\n",
    "            next_q_values = Q_Values.get_next(target_net,next_states)\n",
    "            target_q_values = (next_q_values *(gamma)) + rewards\n",
    "            loss = F.mse_loss(current_q_values,target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad() #zero out gradients to avoid turning towarda a certain direction\n",
    "            loss.backward()\n",
    "            optimizer.step() #back propagate\n",
    "        if np.average(gray) < 20:\n",
    "            #time.sleep(5) #delay\n",
    "            break\n",
    "        #cv2.imshow('screen', np.array(sct_img))\n",
    "        #if (cv2.waitKey(1) & 0xFF) == ord('q'):\n",
    "            #cv2.destroyAllWindows()\n",
    "            #break\n",
    "    if n % 100 == 0:\n",
    "        print(\"updated policy\")\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    exp_rate = min_rate + (max_rate - min_rate)*np.exp(-exp_decay * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
