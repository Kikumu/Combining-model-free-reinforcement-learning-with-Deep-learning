{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from mss import mss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pyautogui\n",
    "import time\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Extraction(Screen Extraction)\n",
    "from PIL import ImageGrab\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "white_template = cv2.imread('templates/white_temp.png',0) \n",
    "blue_template = cv2.imread('templates/blue_temp.png',0)\n",
    "orange_template = cv2.imread('templates/orange_temp.png',0)\n",
    "green_template = cv2.imread('templates/green_temp.png',0)\n",
    "#cv2.imshow('image',green_template)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of blocks 27\n"
     ]
    }
   ],
   "source": [
    "#one scrren\n",
    "bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} #top left corner of screen\n",
    "sct = mss()\n",
    "sct_img = np.array(sct.grab(bounding_box))\n",
    "gray = cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#gray_template = cv2.cvtColor(green_template, cv2.COLOR_BGR2GRAY)\n",
    "#np_scrn = np.array(sct_img)\n",
    "\n",
    "#print( gray_template.shape[::-1])\n",
    "w,h = blue_template.shape[::-1]  #here\n",
    "result = cv2.matchTemplate(gray,blue_template, cv2.TM_CCOEFF_NORMED) #here\n",
    "\n",
    "f = set()\n",
    "loc = np.where(result >=0.9)\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv2.rectangle(sct_img,pt,(pt[0] + w, pt[1] + h),(0,255,0),3)\n",
    "    sensitivity = 100\n",
    "    f.add((round(pt[0]/sensitivity), round(pt[1]/sensitivity)))\n",
    "    \n",
    "found_count = len(f)    \n",
    "print(\"amount of blocks\", found_count)\n",
    "cv2.imshow(\"result\", sct_img)\n",
    "#cv2.imshow('screen', np.array(sct_img))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#screen method 1\n",
    "#img = ImageGrab.grab(bbox=(200,630,850,600)) #bbox specifies specific region (bbox= x,y,width,height *starts top-left)\n",
    "#img_np = np.array(img) #this is the array obtained from conversion\n",
    "#frame = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
    "#cv2.imshow(\"test\", frame)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#one scrren\n",
    "bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} #top left corner of screen\n",
    "sct = mss()\n",
    "#while True:\n",
    "#7 outputs(mouse positions)\n",
    "sct_img = sct.grab(bounding_box)\n",
    "cv2.imshow('screen', np.array(sct_img))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#screen method 2(Current)\n",
    "bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} #top left corner of screen\n",
    "sct = mss()\n",
    "while True:\n",
    "    #7 outputs(mouse positions)\n",
    "    sct_img = sct.grab(bounding_box)\n",
    "    cv2.imshow('screen', np.array(sct_img))\n",
    "    if (cv2.waitKey(1) & 0xFF) == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 outputs\n",
    "def take_actions(action):\n",
    "    if action == 0:\n",
    "        pyautogui.moveTo(640, 200, duration = 0.03) \n",
    "    elif action == 1:\n",
    "        pyautogui.moveTo(740, 200, duration = 0.03) \n",
    "    elif action == 2:\n",
    "        pyautogui.moveTo(840, 200, duration = 0.03)\n",
    "    elif action ==3:\n",
    "        pyautogui.moveTo(940, 200, duration = 0.03) \n",
    "    elif action ==4:\n",
    "        pyautogui.moveTo(1040, 200, duration = 0.03)\n",
    "    elif action ==5:\n",
    "        pyautogui.moveTo(1140, 200, duration = 0.03)\n",
    "    elif action ==6:\n",
    "        pyautogui.moveTo(1240, 200, duration = 0.03)\n",
    "    elif action == 7:\n",
    "        pyautogui.moveTo(1340, 200, duration = 0.03) \n",
    "    elif action == 8:\n",
    "        pyautogui.moveTo(1440, 200, duration = 0.03)\n",
    "    elif action == 9:\n",
    "        pyautogui.moveTo(1540, 200, duration = 0.03)\n",
    "    else:\n",
    "        print(\"Not allowed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=5, action=6, next_state=7, reward=8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Xp = namedtuple('Experience',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "Xp_points = Xp(5,6,7,8)\n",
    "Xp_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------DQN(Fully connected)-----------------------------------------#\n",
    "class DQN (nn.Module):\n",
    "    def __init__(self,img_height, img_width):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(in_features = img_height*img_width*3,out_features=24)\n",
    "        self.layer2      = nn.Linear(in_features = 24, out_features = 32)\n",
    "        self.output      = nn.Linear(in_features = 32, out_features = 10)\n",
    "#--------------------forward pass---------------------------------------------------------------------#\n",
    "    def forward(self,t):\n",
    "        t = t.flatten(start_dim = 1) #select first layer and flatten\n",
    "        t = F.relu(self.input_layer(t))\n",
    "        t = F.relu(self.layer2(t))\n",
    "        t = self.output(t)\n",
    "        #print(t.item())\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate and network setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "policy_net = DQN(600,850).to(device = \"cpu\")\n",
    "target_net = DQN(600,850).to(device = \"cpu\")\n",
    "target_net.load_state_dict(policy_net.state_dict()) #set weights to be the same\n",
    "target_net.eval() #not in training mode\n",
    "optimizer = optim.Adam(params = policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q values setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Values():\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):\n",
    "        #print(\"nx\",next_states)\n",
    "        #print(\"shape\", next_states.shape[0])\n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        #print(\"ft\",final_state_locations)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        #print(\"nf\",non_final_state_locations)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(Q_Values.device)\n",
    "        #values = torch.zeros(2).to(Q_Values.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    batch = Xp(*zip(*experiences))\n",
    "    t1 = torch.cat(batch.state)\n",
    "    #print(\"t1\",t1)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    #print(\"t2\",t2)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    #print(\"t3\",t3)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return t1,t2,t3,t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------DEJAVU----------------------------------------------------------#\n",
    "class ReplayMemory():\n",
    "    def __init__(self,capacity):   \n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            #overwrite old memory\n",
    "            self.memory[self.push_count%self.capacity] = experience\n",
    "        self.push_count+=1\n",
    "    #number of returned memories will be equal to batch size\n",
    "    def sample(self, batch_size):\n",
    "        return rand.sample(self.memory,batch_size)\n",
    "    #sample must be equal to batch size\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory)>=batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon Decay Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Greed():\n",
    "    def __init__(self,start,end,decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    def get_exploration_rate(self,current_step):\n",
    "        return self.end + (self.start - self.end)*math.exp(-1. * current_step*self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_screen_data(screen):\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32)/255\n",
    "        screen = torch.from_numpy(screen).float()\n",
    "        \n",
    "        resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize((600, 850)),\n",
    "                    T.ToTensor()])\n",
    "        return resize(screen).unsqueeze(0).to(device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(100000)\n",
    "gamma = 0.99 #affects network\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "updated policy\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([5])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([5])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([5])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "updated policy\n",
      "rand_value tensor([4])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([5])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([5])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([1.])\n",
      "rewards 0 1\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([2])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([3])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([5])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([7])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tensor tensor([1.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([6])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([1])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([8])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([9])\n",
      "reward_tensor tensor([-3.])\n",
      "reward_tenosr tensor([-4.])\n",
      "rand_value tensor([4])\n"
     ]
    },
    {
     "ename": "FailSafeException",
     "evalue": "PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailSafeException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-de5bbd02c776>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrndm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rand_value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mtake_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-a818e8d3a5d4>\u001b[0m in \u001b[0;36mtake_actions\u001b[1;34m(action)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mpyautogui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveTo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m940\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.03\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpyautogui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveTo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1040\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.03\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mpyautogui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveTo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1140\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.03\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyautogui\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[0mfuncArgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcallargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrappedFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mfailSafeCheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m         \u001b[0mreturnVal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrappedFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0m_handlePause\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuncArgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_pause\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyautogui\\__init__.py\u001b[0m in \u001b[0;36mfailSafeCheck\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1670\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mFAILSAFE\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFAILSAFE_POINTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m         raise FailSafeException(\n\u001b[1;32m-> 1672\u001b[1;33m             \u001b[1;34m\"PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1673\u001b[0m         )\n\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailSafeException\u001b[0m: PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED."
     ]
    }
   ],
   "source": [
    "#def launch():\n",
    "exp_rate = 1\n",
    "episode = 0\n",
    "max_rate = 1\n",
    "min_rate = 0.01\n",
    "exp_decay = 0.000001\n",
    "for n in range (200):\n",
    "    w,h = blue_template.shape[::-1]  #set template shape\n",
    "    count = 0\n",
    "    current_blocks = 0\n",
    "    past_blocks = 0\n",
    "    reward = 0\n",
    "    bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} \n",
    "    sct = mss()\n",
    "    while True:\n",
    "        #7 outputs(mouse positions)\n",
    "        init_random = np.random.uniform(0,1)\n",
    "        past_blocks = current_blocks\n",
    "        count+=1\n",
    "        sct_img =(sct.grab(bounding_box))\n",
    "        #if count < 3:\n",
    "        #    print(\"y\")\n",
    "        x,y = pyautogui.position()\n",
    "        pyautogui.click(x, y)\n",
    "        \n",
    "        #3 frames\n",
    "        first_example = transform_screen_data(sct_img) #Transform screen(current screen)/previous\n",
    "        sct_img =(sct.grab(bounding_box))\n",
    "        second_example = transform_screen_data(sct_img) #Transform screen(current screen)/previous\n",
    "        sct_img =(sct.grab(bounding_box))\n",
    "        third_example = transform_screen_data(sct_img) #Transform screen(current screen)/previous\n",
    "        example = first_example - second_example - third_example\n",
    "        \n",
    "        \n",
    "        gray = cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGR2GRAY) #block detection\n",
    "        if np.average(gray) < 20:\n",
    "            reward = 0\n",
    "            reward_tensor = torch.tensor([reward], device = \"cpu\")\n",
    "            example = torch.zeros_like(example)\n",
    "            \n",
    "        #epsilon greed\n",
    "        exp_thresh = rand.uniform(0,1)\n",
    "        if exp_thresh > exp_rate:\n",
    "             with torch.no_grad():#need to review this\n",
    "                action = policy_net(example).argmax(dim=1).to(device =\"cpu\")\n",
    "                print(\"net_value\", action)\n",
    "        else:\n",
    "            rndm = rand.randrange(1,10)\n",
    "            action = torch.tensor([rndm]).to(device = \"cpu\")\n",
    "            print(\"rand_value\", action)\n",
    "        take_actions(action)\n",
    "        \n",
    "        #reward feed(Block detection)\n",
    "        #gray = cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGR2GRAY)\n",
    "        f = set()\n",
    "        result = cv2.matchTemplate(gray,blue_template, cv2.TM_CCOEFF_NORMED) #match template per frame\n",
    "        loc = np.where(result >=0.9)\n",
    "        for pt in zip(*loc[::-1]):\n",
    "            sensitivity = 100\n",
    "            f.add((round(pt[0]/sensitivity), round(pt[1]/sensitivity)))\n",
    "        found_count = len(f) # reward factor\n",
    "        current_blocks = found_count\n",
    "        if count < 2:\n",
    "            reward = 0\n",
    "            reward_tensor = torch.FloatTensor([reward], device = \"cpu\")\n",
    "        if count > 2:\n",
    "            if(current_blocks < past_blocks):\n",
    "                reward+=1\n",
    "                reward_tensor = torch.FloatTensor([reward], device = \"cpu\")\n",
    "                print(\"reward_tensor\",reward_tensor)\n",
    "        if count %10 == 0:\n",
    "            print(\"rewards\",found_count, reward)\n",
    "            #look for black screen and reset count\n",
    "        #next state\n",
    "        if reward < 2:\n",
    "            reward = -4\n",
    "            reward_tensor = torch.FloatTensor([reward], device = \"cpu\")\n",
    "            print(\"reward_tenosr\", reward_tensor)\n",
    "            \n",
    "        #from that action i got this state\n",
    "        sct_img =(sct.grab(bounding_box))\n",
    "        second_example1 = transform_screen_data(sct_img)\n",
    "        sct_img =(sct.grab(bounding_box))\n",
    "        second_example2 = transform_screen_data(sct_img)\n",
    "        sct_img =(sct.grab(bounding_box))\n",
    "        second_example3 = transform_screen_data(sct_img)\n",
    "        \n",
    "        example1 = second_example1 - second_example2 - second_example3\n",
    "        if np.average(gray) < 20:\n",
    "            reward = 0\n",
    "            reward_tensor = torch.FloatTensor([reward], device = \"cpu\")\n",
    "            example1 = torch.zeros_like(example1)\n",
    "            next_state = example1\n",
    "        if np.average(gray) > 20:\n",
    "            next_state = example1\n",
    "        #screen sub\n",
    "        #print(\"nxt_sts\",next_state)\n",
    "        #print(\"snsxd\",example.type())\n",
    "        memory.push(Xp(example,action,next_state,reward_tensor))\n",
    "        if memory.can_provide_sample(200):\n",
    "            Xps = memory.sample(200)\n",
    "            states, actions,rewards,next_states = extract_tensors(Xps)\n",
    "            #print(\"s\",states)\n",
    "            #print(\"a\",actions)\n",
    "            #print(\"r\",rewards)\n",
    "            #print(\"s\",next_states)\n",
    "            current_q_values = Q_Values.get_current(policy_net,states,actions)\n",
    "            next_q_values = Q_Values.get_next(target_net,next_states)\n",
    "            #print(\"r\",rewards)\n",
    "            #print(\"q\",next_q_values)\n",
    "            #print((rewards.type()))\n",
    "            #print((next_q_values.type()))\n",
    "            #next_q_values = float(next_q_values)\n",
    "            target_q_values = (next_q_values *(gamma)) + rewards\n",
    "            #print(\"here\")\n",
    "            loss = F.mse_loss(current_q_values,target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad() #zero out gradients to avoid turning towarda a certain direction\n",
    "            loss.backward()\n",
    "            optimizer.step() #back propagate\n",
    "        if np.average(gray) < 20:\n",
    "            reward = 0\n",
    "            time.sleep(4) #delay\n",
    "            break\n",
    "        #cv2.imshow('screen', np.array(sct_img))\n",
    "        #if (cv2.waitKey(1) & 0xFF) == ord('q'):\n",
    "            #cv2.destroyAllWindows()\n",
    "            #break\n",
    "    if n % 30 == 0:\n",
    "            print(\"updated policy\")\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    exp_rate = min_rate + (max_rate - min_rate)*np.exp(-exp_decay * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
