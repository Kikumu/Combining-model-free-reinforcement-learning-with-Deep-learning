{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from mss import mss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pyautogui\n",
    "import time\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Extraction(Screen Extraction)\n",
    "from PIL import ImageGrab\n",
    "import cv2\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screen transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_screen_data1(screen):\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32)/255\n",
    "        screen = torch.from_numpy(screen).float()\n",
    "        return screen # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture Screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_screen(box):\n",
    "    sct = mss()\n",
    "    sct_img =(sct.grab(bounding_box))\n",
    "    sct_img = cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGR2GRAY)\n",
    "    first_example = transform_screen_data1(sct_img) #Transform screen(current screen)/previous\n",
    "    #sct_img =(sct.grab(bounding_box))\n",
    "    #sct_img = cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGR2GRAY)\n",
    "    #second_example = transform_screen_data1(sct_img) #Transform screen(current screen)/previous\n",
    "    #example = second_example - first_example\n",
    "    example = first_example\n",
    "    return example\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "white_template = cv2.imread('templates/white_temp.png',0) \n",
    "blue_template = cv2.imread('templates/blue_temp.png',0)\n",
    "orange_template = cv2.imread('templates/orange_temp.png',0)\n",
    "green_template = cv2.imread('templates/green_temp.png',0)\n",
    "cv2.imshow('image',blue_template)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards initial block count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of blocks 21\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "#one scrren\n",
    "bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} #top left corner of screen\n",
    "sct = mss()\n",
    "sct_img = np.array(sct.grab(bounding_box))\n",
    "gray = cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGR2GRAY)\n",
    "#cv2.imshow('image',gray)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "w,h = blue_template.shape[::-1]  #here\n",
    "result = cv2.matchTemplate(gray,blue_template, cv2.TM_CCOEFF_NORMED) #here \n",
    "f = set()\n",
    "loc = np.where(result >=0.9)\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv2.rectangle(sct_img,pt,(pt[0] + w, pt[1] + h),(0,255,0),2)\n",
    "    sensitivity = 100\n",
    "    f.add((round(pt[0]/sensitivity), round(pt[1]/sensitivity)))\n",
    "    \n",
    "found_count = len(f)    \n",
    "print(\"amount of blocks\", found_count)\n",
    "cv2.imshow(\"result\", sct_img)\n",
    "#cv2.imshow('screen', np.array(sct_img))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "initial_block_value = found_count\n",
    "print(initial_block_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing screen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 850])\n",
      "torch.Size([1, 600, 850])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} #top left corner of screen\n",
    "result = capture_screen(bounding_box)\n",
    "cv2.imshow(\"result\", np.array(result))\n",
    "#cv2.imshow('screen', np.array(sct_img))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(result.shape)\n",
    "print(result.squeeze().unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 outputs\n",
    "def take_actions(action):\n",
    "    if action == 0:\n",
    "        pyautogui.moveTo(640, 400, duration = 0.07) \n",
    "    elif action == 1:\n",
    "        pyautogui.moveTo(740, 400, duration = 0.07) \n",
    "    elif action == 2:\n",
    "        pyautogui.moveTo(840, 400, duration = 0.07)\n",
    "    elif action ==3:\n",
    "        pyautogui.moveTo(940, 400, duration = 0.07) \n",
    "    elif action ==4:\n",
    "        pyautogui.moveTo(1040, 400, duration = 0.07)\n",
    "    elif action ==5:\n",
    "        pyautogui.moveTo(1140, 400, duration = 0.07)\n",
    "    elif action ==6:\n",
    "        pyautogui.moveTo(1240, 400, duration = 0.07)\n",
    "    elif action == 7:\n",
    "        pyautogui.moveTo(1340, 400, duration = 0.07) \n",
    "    elif action == 8:\n",
    "        pyautogui.moveTo(1440, 400, duration = 0.07)\n",
    "    elif action == 9:\n",
    "        pyautogui.moveTo(1500, 400, duration = 0.07)\n",
    "    else:\n",
    "        print(\"Not allowed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=5, action=6, next_state=7, reward=8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Xp = namedtuple('Experience',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "Xp_points = Xp(5,6,7,8)\n",
    "Xp_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------DQN(Fully connected)-----------------------------------------#\n",
    "class DQN (nn.Module):\n",
    "    def __init__(self,img_height, img_width):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(in_features = img_height*img_width,out_features=850)\n",
    "        self.layer2      = nn.Linear(in_features = 850, out_features = 200)\n",
    "        self.layer3      = nn.Linear(in_features = 200, out_features = 132)\n",
    "        self.layer4      = nn.Linear(in_features = 132, out_features = 100)\n",
    "        self.output      = nn.Linear(in_features = 100, out_features = 10)\n",
    "#--------------------forward pass---------------------------------------------------------------------#\n",
    "    def forward(self,t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.input_layer(t))\n",
    "        t = F.relu(self.layer2(t))\n",
    "        t = F.relu(self.layer3(t))\n",
    "        t = F.relu(self.layer4(t))\n",
    "        t = self.output(t)\n",
    "        #print(t.item())\n",
    "        #print(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q values setup (fix this up abit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Values():\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        actions = actions.squeeze()\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):\n",
    "        #print(\"nx\",next_states)\n",
    "        #print(\"shape\", next_states.shape[0])\n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        #print(\"ft\",final_state_locations)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        #print(\"nf\",non_final_state_locations)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(Q_Values.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    batch = Xp(*zip(*experiences))\n",
    "    t1 = torch.stack(batch.state) #stack\n",
    "    t2 = torch.stack(batch.action)\n",
    "    t3 = torch.stack(batch.reward)\n",
    "    t4 = torch.stack(batch.next_state)\n",
    "    return t1,t2,t3,t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------DEJAVU----------------------------------------------------------#\n",
    "class ReplayMemory():\n",
    "    def __init__(self,capacity):   \n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            #overwrite old memory\n",
    "            self.memory[self.push_count%self.capacity] = experience\n",
    "        self.push_count+=1\n",
    "    #number of returned memories will be equal to batch size\n",
    "    def sample(self, batch_size):\n",
    "        return rand.sample(self.memory,batch_size)\n",
    "    #sample must be equal to batch size\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory)>=batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(10000)\n",
    "gamma = 0.7 #affects network\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "policy_net = DQN(600,850).to(device = \"cpu\")\n",
    "target_net = DQN(600,850).to(device = \"cpu\")\n",
    "target_net.load_state_dict(policy_net.state_dict()) #set weights to be the same\n",
    "target_net.eval() #not in training mode\n",
    "optimizer = optim.Adam(params = policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "    \n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\", \\\n",
    "          moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_strategy(threshold, rate, policy_net, state):\n",
    "    if threshold > rate:\n",
    "        with torch.no_grad():#need to review this\n",
    "            action = policy_net(state).argmax(dim=1).to(device =\"cpu\")#argmat was -1\n",
    "            print(\"network\",action)\n",
    "    else:\n",
    "        rndm = rand.randrange(0,9)\n",
    "        action = torch.tensor([rndm]).to(device = \"cpu\")\n",
    "        print(\"rand_value\", action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_count():\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks 20 past_blocks 21\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 20 past_blocks1 20\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks 19 past_blocks 20\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 19 past_blocks1 19\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 19 past_blocks1 19\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 19 past_blocks1 19\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 19 past_blocks1 19\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks 18 past_blocks 19\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 18 past_blocks1 18\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 18 past_blocks1 18\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 18 past_blocks1 18\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 18 past_blocks1 18\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 18 past_blocks1 18\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 18 past_blocks1 18\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 18 past_blocks1 18\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 18 past_blocks1 18\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks 17 past_blocks 18\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 17 past_blocks1 17\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks 16 past_blocks 17\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks 15 past_blocks 16\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 15 past_blocks1 15\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 15 past_blocks1 15\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks 14 past_blocks 15\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 14 past_blocks1 14\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks 13 past_blocks 14\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 13 past_blocks1 13\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 13 past_blocks1 13\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 13 past_blocks1 13\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 13 past_blocks1 13\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 13 past_blocks1 13\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks 12 past_blocks 13\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 12 past_blocks1 12\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 12 past_blocks1 12\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 12 past_blocks1 12\n",
      "reward_tensor1 tensor([-1.])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 12 past_blocks1 12\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: Using a target size (torch.Size([200, 1, 200])) that is different to the input size (torch.Size([200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "click\n",
      "rand_value tensor([4])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "updated policy\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks 20 past_blocks 21\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([0])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([2])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([3])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([2])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([4])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks 20 past_blocks 21\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks 19 past_blocks 20\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([8])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([3])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([6])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "network tensor([6])\n",
      "current_blocks 19 past_blocks 21\n",
      "sub 2\n",
      "reward_tensor0 tensor([2.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks 17 past_blocks 19\n",
      "sub 2\n",
      "reward_tensor0 tensor([2.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "network tensor([2])\n",
      "current_blocks 14 past_blocks 17\n",
      "sub 3\n",
      "reward_tensor0 tensor([3.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks 12 past_blocks 14\n",
      "sub 2\n",
      "reward_tensor0 tensor([2.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks 10 past_blocks 12\n",
      "sub 2\n",
      "reward_tensor0 tensor([2.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "network tensor([4])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "network tensor([8])\n",
      "current_blocks 20 past_blocks 21\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([4])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([8])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks 20 past_blocks 21\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([6])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks 19 past_blocks 21\n",
      "sub 2\n",
      "reward_tensor0 tensor([2.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks 17 past_blocks 19\n",
      "sub 2\n",
      "reward_tensor0 tensor([2.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([6])\n",
      "current_blocks 14 past_blocks 17\n",
      "sub 3\n",
      "reward_tensor0 tensor([3.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "network tensor([7])\n",
      "current_blocks 13 past_blocks 14\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks 10 past_blocks 13\n",
      "sub 3\n",
      "reward_tensor0 tensor([3.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([5])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([6])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks 19 past_blocks 21\n",
      "sub 2\n",
      "reward_tensor0 tensor([2.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks 17 past_blocks 19\n",
      "sub 2\n",
      "reward_tensor0 tensor([2.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks 14 past_blocks 17\n",
      "sub 3\n",
      "reward_tensor0 tensor([3.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks 13 past_blocks 14\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks 10 past_blocks 13\n",
      "sub 3\n",
      "reward_tensor0 tensor([3.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "network tensor([2])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([8])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([3])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([8])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([0])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([5])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([5])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([1])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([2])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([4])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([7])\n",
      "current_blocks 20 past_blocks 21\n",
      "sub 1\n",
      "reward_tensor0 tensor([1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([1])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([2])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "click\n",
      "rand_value tensor([1])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n",
      "Exiting loop...\n",
      "initial state torch.Size([1, 600, 850])\n",
      "click\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network tensor([7])\n",
      "current_blocks1 21 past_blocks1 21\n",
      "reward_tensor1 tensor([-1.])\n",
      "sampling\n",
      "Computed Current\n",
      "Q complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f2480a8438f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_q_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_q_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#zero out gradients to avoid turning towarda a certain direction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#back propagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def launch():\n",
    "exp_rate = 1\n",
    "episode = 0\n",
    "max_rate = 1\n",
    "min_rate = 0.01\n",
    "exp_decay = 0.01\n",
    "reward_count = 0\n",
    "for n in range (200):\n",
    "    w,h = blue_template.shape[::-1]  #set template shape\n",
    "    count = 0\n",
    "    current_blocks = initial_block_value\n",
    "    past_blocks = initial_block_value\n",
    "    reward = 0\n",
    "    time.sleep(7) #delay\n",
    "    bounding_box = {'top': 200, 'left': 630, 'width': 850, 'height': 600} #top left corner of screen\n",
    "    state = capture_screen(bounding_box)\n",
    "    state = state.squeeze().unsqueeze(dim=0) #important\n",
    "    print(\"initial state\", state.shape)\n",
    "    done = False\n",
    "    while done is not True:\n",
    "        init_random = np.random.uniform(0,1)\n",
    "        past_blocks = current_blocks\n",
    "        count+=1\n",
    "        print(\"click\")\n",
    "        x,y = pyautogui.position()\n",
    "        pyautogui.click(x, y)\n",
    "###############################################ACTION########################################################\n",
    "        #epsilon greed\n",
    "        exp_thresh = rand.uniform(0,1)\n",
    "        action = greedy_strategy(exp_thresh,exp_rate,policy_net,state)\n",
    "        take_actions(action)\n",
    "#############################################REWARD########################################################### \n",
    "        sct_img =(sct.grab(bounding_box))\n",
    "        gray = cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGR2GRAY) #block detection\n",
    "        f = set()\n",
    "        result = cv2.matchTemplate(gray,blue_template, cv2.TM_CCOEFF_NORMED) #match template per frame\n",
    "        loc = np.where(result >=0.9)\n",
    "        for pt in zip(*loc[::-1]):\n",
    "            sensitivity = 100\n",
    "            f.add((round(pt[0]/sensitivity), round(pt[1]/sensitivity)))\n",
    "        found_count = len(f) # reward factor\n",
    "        current_blocks = found_count\n",
    "        #reward feed to network(only give reard if resulting action is not a black screen)\n",
    "        if np.average(gray) > 20:\n",
    "            if(current_blocks < past_blocks):\n",
    "                print(\"current_blocks\", current_blocks,\"past_blocks\",past_blocks )\n",
    "                reward =(past_blocks - current_blocks)\n",
    "                reward_count+=reward\n",
    "                print(\"sub\",reward)\n",
    "                reward = torch.FloatTensor([reward], device = \"cpu\")\n",
    "                print(\"reward_tensor0\",reward)\n",
    "                \n",
    "            else:\n",
    "                print(\"current_blocks1\", current_blocks,\"past_blocks1\",past_blocks )\n",
    "                reward = -1\n",
    "                reward_count+=reward\n",
    "                reward = torch.FloatTensor([reward], device = \"cpu\")\n",
    "                print(\"reward_tensor1\",reward)\n",
    "        else:\n",
    "            done = True\n",
    "            #print(\"its dark\")\n",
    "            reward = -4\n",
    "            reward = torch.FloatTensor([reward], device = \"cpu\")\n",
    "        \n",
    "#########################################NEXT_STATE############################################################        \n",
    "        # if resulting action is a black screen\n",
    "        if(np.average(gray) < 20):\n",
    "            screen_return = torch.zeros_like(state)\n",
    "            done = True\n",
    "        else:\n",
    "            screen_return = capture_screen(bounding_box)\n",
    "        next_state = screen_return\n",
    "        next_state = next_state.squeeze().unsqueeze(dim=0) #important(converts it into necesarry dimensions)\n",
    "        memory.push(Xp(state,action,next_state,reward))\n",
    "        state = next_state\n",
    "######################################EXPERIENCE AND Q VALUES##################################################\n",
    "        if memory.can_provide_sample(200):\n",
    "            Xps = memory.sample(200)\n",
    "            print(\"sampling\")\n",
    "            states, actions,rewards,next_states = extract_tensors(Xps)\n",
    "            current_q_values = Q_Values.get_current(policy_net,states,actions)\n",
    "            print(\"Computed Current\")\n",
    "            next_q_values = Q_Values.get_next(target_net,next_states)\n",
    "            target_q_values = (next_q_values *(gamma)) + rewards\n",
    "            print(\"Q complete\")\n",
    "            loss = F.mse_loss(current_q_values,target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad() #zero out gradients to avoid turning towarda a certain direction\n",
    "            loss.backward()\n",
    "            optimizer.step() #back propagate\n",
    "        if np.average(gray) < 20:\n",
    "            done = True\n",
    "            print(\"Exiting loop...\")\n",
    "            \n",
    "    if n % 100 == 0:\n",
    "        print(\"updated policy\")\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    exp_rate = min_rate + (max_rate - min_rate)*np.exp(-exp_decay * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch()\n",
    "import csv\n",
    "with open('Qtables.csv', 'w') as f:\n",
    "    for i in range(len(memory.memory)):\n",
    "        out_str = \"\"\n",
    "        out_str+=str(memory.memory[i])\n",
    "        out_str +=\"\\n\"\n",
    "        f.write(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Qtables.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
